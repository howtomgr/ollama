{
  "category": "artificial-intelligence",
  "created_date": "2025-09-16T00:00:00.000000",
  "default_ports": [11434],
  "dependencies": ["curl", "systemd"],
  "description": "Ollama is a free and open-source tool for running large language models (LLMs) locally on your machine. It serves as a FOSS alternative to cloud-based AI services like OpenAI API, Anthropic Claude API, or Google's Gemini API, enabling privacy-focused AI deployment, offline inference, and cost-effective local AI processing.",
  "difficulty_level": "intermediate",
  "documentation_url": "https://howtomgr.github.io/artificial-intelligence/ollama",
  "estimated_setup_time": "15-30 minutes",
  "features": [
    "multi-os-support",
    "local-llm-inference",
    "gpu-acceleration",
    "model-management",
    "rest-api",
    "privacy-focused",
    "offline-capable",
    "comprehensive-documentation",
    "security-hardening",
    "performance-optimization",
    "backup-restore-procedures",
    "troubleshooting-guides"
  ],
  "guide_path": "/artificial-intelligence/ollama",
  "installation_methods": [
    "official-installer",
    "package-manager",
    "manual-binary"
  ],
  "last_updated": "2025-09-16T00:00:00.000000",
  "license": "MIT",
  "maintenance_status": "active",
  "name": "ollama",
  "repository_url": "https://github.com/howtomgr/ollama",
  "spec_version": "2.0",
  "subcategory": "llm-runners",
  "supported_os": [
    "rhel",
    "centos",
    "rocky",
    "almalinux",
    "debian",
    "ubuntu",
    "arch",
    "alpine",
    "opensuse",
    "sles",
    "macos",
    "windows"
  ],
  "tags": [
    "ai",
    "llm",
    "machine-learning",
    "local-inference",
    "privacy",
    "openai-alternative",
    "gpu-acceleration",
    "model-serving",
    "rest-api"
  ],
  "title": "Ollama Installation Guide",
  "version": "1.0.0",
  "website_url": "https://howtomgr.github.io/artificial-intelligence/ollama"
}